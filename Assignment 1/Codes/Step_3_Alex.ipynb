{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Step 3) Linear Regression",
   "id": "5344a53177b851a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "9133a49333d686f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:56.148183Z",
     "start_time": "2024-09-28T16:56:56.143756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly_resampler import FigureResampler, FigureWidgetResampler\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "d2737d35a9442204",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data",
   "id": "68db2366b791dfdb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Paths",
   "id": "a5bdd68c5e0dd15e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:56.179834Z",
     "start_time": "2024-09-28T16:56:56.177309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Bornholm_Network_Manager = r\"/home/adb/Documents/DTU/3rdTerm/MLES_Course/Bornholm Network Manager 23-09-2024 21-05-29.csv\"\n",
    "DMI_dataset = r\"/home/adb/Documents/DTU/3rdTerm/MLES_Course/DMI Bornholm 24-09-2024 01-07-21.csv\"\n",
    "Energinet_dataset = r\"/home/adb/Documents/DTU/3rdTerm/MLES_Course/Energiner Balancing Prices 24-09-2024 01-09-47.csv\"\n",
    "Forcast_norwegian_dataset = r\"/home/adb/Documents/DTU/3rdTerm/MLES_Course/Norwegian Forecast dataset.csv\""
   ],
   "id": "a17dce70720e1e74",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Name columns",
   "id": "cd725151d447d8a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:56.185488Z",
     "start_time": "2024-09-28T16:56:56.181277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "time_cl = pl.col('time')\n",
    "kalby_active_power = pl.col('804120')\n",
    "#weather data\n",
    "max_temp = pl.col('406464')\n",
    "mean_temp = pl.col('406432')\n",
    "min_temp = pl.col('406480')\n",
    "accum_precip = pl.col('406656')\n",
    "mean_wind_speed = pl.col('406640')\n",
    "mean_wind_dirn = pl.col('406496')\n",
    "mean_humidity = pl.col('406448')\n",
    "#forecast data\n",
    "fr_wind_dirn = pl.col('128270')\n",
    "fr_accum_precip = pl.col('128238')\n",
    "fr_mean_humidity = pl.col('128254')\n",
    "fr_wind_speed = pl.col('128286')\n",
    "fr_mean_temp = pl.col('128190')\n",
    "#create column mappings\n",
    "prev_day_power = pl.col('prev_day_power')\n",
    "\n",
    "weekly_5th_quantile = pl.col(\"5thQuantile\")\n",
    "weekly_50th_quantile = pl.col(\"50thQuantile\")\n",
    "weekly_90th_quantile = pl.col(\"90thQuantile\")\n",
    "\n",
    "hourly_5th_quantile = pl.col(\"Hour_5thQuantile\")\n",
    "hourly_50th_quantile = pl.col(\"Hour_50thQuantile\")\n",
    "hourly_90th_quantile = pl.col(\"Hour_90thQuantile\")"
   ],
   "id": "441166a90b037741",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Data nad Merging",
   "id": "1b11e05806a91d94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:56.982131Z",
     "start_time": "2024-09-28T16:56:56.186467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wind_data = pl.read_csv(Bornholm_Network_Manager , separator= ',')\n",
    "\n",
    "wind_data = wind_data.with_columns(\n",
    "    pl.col('ts').str.to_datetime(\"%Y-%m-%d %H:%M:%S\").alias('ts')\n",
    ")\n",
    "wind_data = wind_data.sort(pl.col('ts'))\n",
    "\n",
    "simplified_columns = {}\n",
    "new_col_wind = {} \n",
    "for c in wind_data.columns:\n",
    "\tnew_name = c.split(\"|\")[-1].strip()\n",
    "\tsimplified_columns[new_name] = c\n",
    "\tnew_col_wind[c] = new_name\n",
    " \n",
    "wind_data = wind_data.rename(new_col_wind)\n",
    "wind_data = wind_data.with_columns(\n",
    "    pl.col(\"ts\").dt.truncate(\"1h\").alias(\"time\")\n",
    ")\n",
    "wind_data = wind_data.group_by('time').agg(pl.all().mean())"
   ],
   "id": "707a0fb1a1d70c9",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:56.997493Z",
     "start_time": "2024-09-28T16:56:56.983088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dmi = pl.read_csv(DMI_dataset, separator= ',', ignore_errors= True)\n",
    "dmi = dmi.select(\n",
    "    pl.col('ts').str.to_datetime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    pl.exclude('ts')\n",
    ")\n",
    "dmi = dmi.sort(pl.col('ts'))\n",
    "\n",
    "simplified_columns = {}\n",
    "new_col_dmi = {} \n",
    "for c in dmi.columns:\n",
    "\tnew_name = c.split(\"|\")[-1].strip()\n",
    "\tsimplified_columns[new_name] = c\n",
    "\tnew_col_dmi[c] = new_name\n",
    "\n",
    "dmi = dmi.rename(new_col_dmi)\n",
    "dmi = dmi.with_columns(\n",
    "    pl.col(\"ts\").dt.truncate(\"1h\").alias(\"time\")\n",
    ")\n",
    "dmi = dmi.group_by('time').agg(pl.all().mean())"
   ],
   "id": "5e47cc85e6ea5f9e",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:57.012352Z",
     "start_time": "2024-09-28T16:56:56.998944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "energinet = pl.read_csv(Energinet_dataset, separator=',', ignore_errors=True)\n",
    "\n",
    "# Simplify column names\n",
    "simplified_columns = {}\n",
    "new_col_energinet = {}\n",
    "\n",
    "for c in energinet.columns:\n",
    "    new_name = c.split(\"|\")[-1].strip()\n",
    "    simplified_columns[new_name] = c\n",
    "    new_col_energinet[c] = new_name\n",
    "\n",
    "# Rename columns to their simplified names\n",
    "energinet = energinet.rename(new_col_energinet)\n",
    "\n",
    "# Ensure datetime-related columns are of string type\n",
    "energinet = energinet.with_columns([\n",
    "    pl.col('ts').cast(pl.Utf8),\n",
    "    pl.col('804694').cast(pl.Utf8),\n",
    "    pl.col('804695').cast(pl.Utf8)\n",
    "])\n",
    "\n",
    "# Convert the columns to datetime format\n",
    "energinet = energinet.with_columns([\n",
    "    pl.col('ts').str.to_datetime(\"%Y-%m-%d %H:%M:%S\").alias('ts'),\n",
    "    pl.col('804694').str.to_datetime(\"%Y-%m-%d %H:%M:%S\").alias('804694'),\n",
    "    pl.col('804695').str.to_datetime(\"%Y-%m-%d %H:%M:%S\").alias('804695')\n",
    "])\n",
    "\n",
    "# Replace commas with periods for non-date columns and cast to Float64\n",
    "# Cast non-date columns to string first to perform the replacement\n",
    "non_date_columns = [col for col in energinet.columns if col not in ['ts', '804694', '804695', '804696']]\n",
    "\n",
    "# Convert non-date columns to string to allow string operations\n",
    "energinet = energinet.with_columns([\n",
    "    pl.col(column).cast(pl.Utf8) for column in non_date_columns\n",
    "])\n",
    "\n",
    "# Perform string replacement and cast to Float64\n",
    "energinet = energinet.with_columns([\n",
    "    pl.col(column).str.replace(\",\", \".\").cast(pl.Float64, strict=False) for column in non_date_columns\n",
    "])\n",
    "\n",
    "# Filter rows based on specific condition\n",
    "energinet = energinet.filter(pl.col('804696') == 'DK2')\n",
    "\n",
    "# Rename 'ts' column to 'time'\n",
    "energinet = energinet.rename({'ts': 'time'})\n",
    "\n",
    "# Truncate the 'time' column to hourly precision and add it back as a new column\n",
    "energinet = energinet.with_columns(\n",
    "    pl.col(\"time\").dt.truncate(\"1h\").alias(\"time\")\n",
    ")\n",
    "\n",
    "# Group by 'time' and calculate the mean for all other columns\n",
    "energinet = energinet.group_by('time').agg(pl.all().mean())"
   ],
   "id": "aa883da3bd1b6d05",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:57.020950Z",
     "start_time": "2024-09-28T16:56:57.013355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "forecast = pl.read_csv(Forcast_norwegian_dataset , separator= ',', ignore_errors= True)\n",
    "\n",
    "forecast = forecast.select(\n",
    "    pl.col('ts').str.to_datetime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    pl.exclude('ts')\n",
    ")\n",
    "simplified_columns = {}\n",
    "new_col_forecast = {} \n",
    "for c in forecast.columns:\n",
    "\tnew_name = c.split(\"|\")[-1].strip()\n",
    "\tsimplified_columns[new_name] = c\n",
    "\tnew_col_forecast[c] = new_name\n",
    "\n",
    "forecast = forecast.rename(new_col_forecast)\n",
    "\n",
    "forecast = forecast.rename({'ts':'time'})\n",
    "forecast = forecast.with_columns(\n",
    "    pl.col(\"time\").dt.truncate(\"1h\").alias(\"time\")\n",
    ")\n",
    "forecast = forecast.group_by('time').agg(pl.all().mean())"
   ],
   "id": "a9775db535f5550f",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:57.028765Z",
     "start_time": "2024-09-28T16:56:57.021641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# merge all datasets into one\n",
    "temp1 = wind_data.join(dmi, on = 'time', how = 'left', validate= 'm:1')\n",
    "cumulative_dataset = temp1.join(forecast, on = 'time', how = 'left', validate= 'm:1')\n",
    "#cumulative_dataset = temp2.join(energinet, on='time', how = 'left', validate = 'm:1')\n",
    "cumulative_dataset = cumulative_dataset.drop('ts', 'ts_right')\n",
    "cumulative_dataset = cumulative_dataset.drop_nulls(subset = ['804120'])\n",
    "cumulative_dataset = cumulative_dataset.with_columns(kalby_active_power.shift(24).alias('prev_day_power'))"
   ],
   "id": "9cc5b28e7f61b923",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adding Quantiles",
   "id": "29b20369c37cad80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:57.033821Z",
     "start_time": "2024-09-28T16:56:57.029535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#quantile of the week ahead of that data point\n",
    "cumulative_dataset = cumulative_dataset.sort(\"time\")\n",
    "df = cumulative_dataset.rolling(index_column= 'time', period='1w').agg([\n",
    "        pl.quantile(\"804120\", 0.05).alias(\"5thQuantile\"),\n",
    "        pl.quantile(\"804120\", 0.50).alias(\"50thQuantile\"),\n",
    "        pl.quantile(\"804120\", 0.95).alias(\"90thQuantile\"),\n",
    "])\n",
    "cumulative_dataset = cumulative_dataset.join(df, on=\"time\", how=\"left\")"
   ],
   "id": "944839acded75216",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:56:57.059487Z",
     "start_time": "2024-09-28T16:56:57.034517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#quantile of the hour of the day for the previous week\n",
    "cumulative_dataset = cumulative_dataset.with_columns(pl.col(\"time\").dt.hour().alias(\"hour_of_day\"))\n",
    "cumulative_dataset = cumulative_dataset.with_columns([\n",
    "    pl.lit(None).cast(pl.Float64).alias(\"Hour_5thQuantile\"),\n",
    "    pl.lit(None).cast(pl.Float64).alias(\"Hour_50thQuantile\"),\n",
    "    pl.lit(None).cast(pl.Float64).alias(\"Hour_90thQuantile\"),\n",
    "])\n",
    "rolling_quantile_hourly = pl.DataFrame()\n",
    "for hour in range(24):  \n",
    "    hour_df = cumulative_dataset.filter(pl.col(\"time\").dt.hour() == hour)\n",
    "    rolling_quantiles = (hour_df.rolling(index_column=\"time\", period=\"1w\").agg([\n",
    "            pl.quantile(\"804120\", 0.05).alias(\"Hour_5thQuantile\"),\n",
    "            pl.quantile(\"804120\", 0.50).alias(\"Hour_50thQuantile\"),\n",
    "            pl.quantile(\"804120\", 0.95).alias(\"Hour_90thQuantile\"),\n",
    "        ]))\n",
    "    rolling_quantile_hourly = pl.concat([rolling_quantile_hourly, rolling_quantiles], how=\"vertical\")\n",
    "rolling_quantile_hourly = rolling_quantile_hourly.sort(\"time\")\n",
    "\n",
    "for quantile_col in [\"Hour_5thQuantile\", \"Hour_50thQuantile\", \"Hour_90thQuantile\"]:\n",
    "    cumulative_dataset = cumulative_dataset.with_columns(\n",
    "        pl.when(cumulative_dataset[\"time\"].is_in(rolling_quantile_hourly[\"time\"]))\n",
    "          .then(rolling_quantile_hourly[quantile_col])\n",
    "          .otherwise(pl.col(quantile_col)).alias(quantile_col)\n",
    "    )"
   ],
   "id": "c84898b9d083fa31",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Linear Regression - Gradien descent algorithm",
   "id": "3c5c881701b20d10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Linear Regression",
   "id": "a90386b9672d9167"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T17:54:09.683839Z",
     "start_time": "2024-09-28T17:54:09.678300Z"
    }
   },
   "cell_type": "code",
   "source": "cumulative_dataset",
   "id": "d4e6f24fe2f2ebe",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T17:54:06.081277Z",
     "start_time": "2024-09-28T17:54:06.077952Z"
    }
   },
   "cell_type": "code",
   "source": "cumulative_dataset = cumulative_dataset.drop_nulls()",
   "id": "5d9ce9408067d44c",
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T17:52:44.014150Z",
     "start_time": "2024-09-28T17:52:44.002010Z"
    }
   },
   "cell_type": "code",
   "source": "cumulative_dataset.drop_na()",
   "id": "98592cf047535261",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Linear regression aims to model the relationship between a dependent variable (target = wind power production) y, and several independent variables (features) X. The goal is to find the best-fitting linear equation that predicts y based on X.\n",
    "\n",
    "$$\n",
    "y = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\ldots + \\theta_n X_n\n",
    "$$\n",
    "\n",
    "theta 0 is the bias thetas are the weights"
   ],
   "id": "d6f052b8773f25fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T17:45:13.887094Z",
     "start_time": "2024-09-28T17:45:13.871133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = cumulative_dataset.drop(['804120']) # Features (excluding the target column)\n",
    "y = cumulative_dataset['804120'].reshape(-1, 1) # Target variable\n",
    "\n",
    "# Add a column of ones to include the bias term (intercept) in the calculation\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X] # X_b = [[1, x1], [1, x2], ..., [1, xn]]"
   ],
   "id": "3c8f5364327bd252",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T17:45:40.575419Z",
     "start_time": "2024-09-28T17:45:40.563102Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "2780b93784b1447b",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It is basen on minimising the mean squared error",
   "id": "55ad68533adcf47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def MSE(X, y, theta):\n",
    "    predictions = X.dot(theta)\n",
    "    errors = predictions - y\n",
    "    return (1 / 2) * np.sum(errors ** 2)"
   ],
   "id": "47c0c90960b1060c",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "3693e290d085fab1",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-28T17:54:49.856327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming cumulative_dataset is your Polars DataFrame\n",
    "\n",
    "# Extract features and target from the Polars DataFrame\n",
    "X = cumulative_dataset.drop('804120').to_numpy()  # Features (excluding the target column)\n",
    "y = cumulative_dataset['804120'].to_numpy().reshape(-1, 1)  # Target variable reshaped to be a column vector\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add a column of ones to include the bias term (intercept) in the calculation\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]  # X_b = [[1, x1], [1, x2], ..., [1, xn]]\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001  # Reduced learning rate\n",
    "n_iterations = 1000    # Number of iterations\n",
    "m = len(X_b)           # Number of samples\n",
    "\n",
    "# Initialize weights with smaller random values\n",
    "theta = np.random.randn(X_b.shape[1], 1) * 0.01  # Initialize theta for all features including the bias term\n",
    "\n",
    "# Cost Function\n",
    "def compute_cost(X, y, theta):\n",
    "    predictions = X.dot(theta)\n",
    "    errors = predictions - y\n",
    "    return (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "\n",
    "# Gradient Descent Function\n",
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    cost_history = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = (1 / m) * X.T.dot(X.dot(theta) - y)\n",
    "        theta = theta - learning_rate * gradients\n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Optional: print cost every 100 iterations for tracking\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}, Cost: {cost}\")\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "# Train the model\n",
    "theta_best, cost_history = gradient_descent(X_b, y, theta, learning_rate, n_iterations)\n",
    "\n",
    "# Print the resulting parameters\n",
    "print(\"Best-fit parameters (theta):\", theta_best)\n",
    "\n"
   ],
   "id": "45b152c7abdba1c5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plotting the cost history\n",
    "plt.plot(range(n_iterations), cost_history)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost Function Reduction Over Time\")\n",
    "plt.show()\n"
   ],
   "id": "62faf2c665e4eaa8",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
